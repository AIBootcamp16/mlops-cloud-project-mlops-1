# src/pipeline/tasks/fe.py - ìˆ˜ì •ëœ ë²„ì „
# -*- coding: utf-8 -*-
"""
Feature engineering task - ê°œì„ ëœ ë²„ì „
- ë” ê°•ë ¥í•œ ë°ì´í„° ë¡œë”© ë¡œì§
- í´ë°± ë©”ì»¤ë‹ˆì¦˜ ì¶”ê°€
- ì—ëŸ¬ ì²˜ë¦¬ ê°œì„ 
"""

import io
import boto3
import pandas as pd
import os, sys, argparse
from urllib.parse import urlparse
from datetime import datetime

PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../.."))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)


def load_processed_data_safe(tracking_uri=None):
    """ì•ˆì „í•˜ê²Œ ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ"""
    import mlflow
    import pandas as pd
    from mlflow.tracking import MlflowClient

    if tracking_uri:
        mlflow.set_tracking_uri(tracking_uri)

    try:
        # 1ë‹¨ê³„: MLflowCovidPipeline ì‹œë„ (ê°„ë‹¨í•œ ë©”ì„œë“œë§Œ)
        try:
            from src.pipeline.mlflow_pipeline import MLflowCovidPipeline
            pipeline = MLflowCovidPipeline(tracking_uri=tracking_uri)

            # ê°„ë‹¨í•œ ë©”ì„œë“œ ì‹œë„
            if hasattr(pipeline, "load_latest_processed"):
                processed = pipeline.load_latest_processed()
                if processed is not None:
                    print(f"[fe] loaded via MLflowCovidPipeline: {processed.shape}")
                    return processed
        except Exception as e:
            print(f"[fe] MLflowCovidPipeline failed: {e}")

        # 2ë‹¨ê³„: ì§ì ‘ MLflowì—ì„œ ì „ì²˜ë¦¬ ì‹¤í—˜ ê²€ìƒ‰
        print("[fe] Trying direct MLflow search...")
        client = MlflowClient()

        # ì „ì²˜ë¦¬ ì‹¤í—˜ë“¤ ê²€ìƒ‰
        preprocessing_experiments = ["covid_data_preprocessing"]

        for exp_name in preprocessing_experiments:
            try:
                exp = client.get_experiment_by_name(exp_name)
                if not exp:
                    continue

                # ìµœê·¼ ì„±ê³µí•œ ëŸ°ë“¤ ê²€ìƒ‰
                runs_df = mlflow.search_runs(
                    [exp.experiment_id],
                    filter_string="attribute.status = 'FINISHED'",
                    order_by=["start_time DESC"],
                    max_results=10
                )

                if runs_df.empty:
                    print(f"[fe] No FINISHED runs in {exp_name}")
                    continue

                # ê° ëŸ°ì—ì„œ processed_data ì•„í‹°íŒ©íŠ¸ ì°¾ê¸°
                for _, run in runs_df.iterrows():
                    run_id = run["run_id"]
                    try:
                        # processed_data í´ë”ì—ì„œ CSV íŒŒì¼ ì°¾ê¸°
                        artifact_paths = ["processed_data", "data", "artifacts"]

                        for artifact_path in artifact_paths:
                            try:
                                dir_path = mlflow.artifacts.download_artifacts(f"runs:/{run_id}/{artifact_path}")
                                csv_files = [f for f in os.listdir(dir_path) if f.endswith(".csv")]

                                if csv_files:
                                    csv_file = os.path.join(dir_path, sorted(csv_files)[-1])  # ìµœì‹  íŒŒì¼
                                    processed_data = pd.read_csv(csv_file)

                                    # ê¸°ë³¸ì ì¸ ê²€ì¦
                                    if len(processed_data) > 0 and 'date' in processed_data.columns:
                                        print(f"[fe] loaded from run {run_id}: {processed_data.shape}")
                                        return processed_data

                            except Exception as e:
                                print(f"[fe] artifact path {artifact_path} failed: {e}")
                                continue

                    except Exception as e:
                        print(f"[fe] run {run_id} failed: {e}")
                        continue

            except Exception as e:
                print(f"[fe] experiment {exp_name} failed: {e}")
                continue

        print("[fe] No processed data found in any preprocessing run")
        return None

    except Exception as e:
        print(f"[fe] Data loading failed: {e}")
        return None

# src/pipeline/tasks/fe.py (í•µì‹¬ ì¶”ê°€/ë³€ê²½ë§Œ)


def _save_csv_to_s3(df, s3_uri: str) -> str:
    """
    s3_uriê°€ 's3://bucket/prefix/'(í”„ë¦¬í”½ìŠ¤)ë©´
    s3://bucket/prefix/covid_features_{ts}.csv ë¡œ ì €ì¥.
    íŒŒì¼ëª…(.csv)ë¡œ ëë‚˜ë©´ ê·¸ëŒ€ë¡œ ì €ì¥.
    """
    u = urlparse(s3_uri)
    assert u.scheme == "s3", f"Invalid S3 URI: {s3_uri}"
    bucket = u.netloc
    key = u.path.lstrip("/")
    if not key or key.endswith("/"):
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        key = key.rstrip("/") + f"/covid_features_{ts}.csv"

    buf = io.StringIO()
    df.to_csv(buf, index=False)
    boto3.client("s3").put_object(
        Bucket=bucket,
        Key=key,
        Body=buf.getvalue().encode("utf-8"),
        ContentType="text/csv",
    )
    return f"s3://{bucket}/{key}"


def create_dummy_processed_data():
    """ë”ë¯¸ ì „ì²˜ë¦¬ ë°ì´í„° ìƒì„±"""
    import pandas as pd
    import numpy as np

    print("[fe] Creating dummy processed data...")

    dates = pd.date_range(start='2024-01-01', end='2024-12-31', freq='D')
    dummy_data = pd.DataFrame({
        'date': dates,
        'new_cases': np.random.randint(100, 1000, len(dates)),
        'total_cases': np.cumsum(np.random.randint(100, 1000, len(dates))),
        'new_deaths': np.random.randint(0, 50, len(dates)),
        'total_deaths': np.cumsum(np.random.randint(0, 50, len(dates))),
        'stringency_index': np.random.uniform(0, 100, len(dates)),
        'reproduction_rate': np.random.uniform(0.5, 2.0, len(dates)),

        # ì´ë¯¸ ì „ì²˜ë¦¬ëœ í˜•íƒœë¡œ ìƒì„±
        'year': dates.year,
        'month': dates.month,
        'day': dates.day,
        'day_of_week': dates.dayofweek,
        'is_weekend': dates.dayofweek.isin([5, 6]).astype(int),
    })

    return dummy_data

def _slice_last_n_days(df, n_days: int, date_col: str = "date") -> pd.DataFrame:
    if date_col not in df.columns:
        raise ValueError(f"'{date_col}' column is required")
    df = df.copy()
    df[date_col] = pd.to_datetime(df[date_col], errors="coerce")
    df = df.dropna(subset=[date_col]).sort_values(date_col)
    end = df[date_col].max()
    start = end - pd.Timedelta(days=n_days - 1)
    return df[(df[date_col] >= start) & (df[date_col] <= end)].reset_index(drop=True)

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--run-name", type=str, default="feature_engineering_v1")
    ap.add_argument("--target", type=str, default="new_cases")
    ap.add_argument("--tracking-uri", type=str, default=None)
    ap.add_argument("--output", type=str, default="/tmp/features.csv")  # â­ ì¶”ê°€
    ap.add_argument("--train-window-days", type=int, default=365, help="ìµœê·¼ Nì¼ êµ¬ê°„ìœ¼ë¡œ ìŠ¬ë¼ì´ìŠ¤")
    args = ap.parse_args()

    print(f"[fe] Starting feature engineering task...")
    print(f"[fe] Target: {args.target}")

    # ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ ì‹œë„
    processed = load_processed_data_safe(tracking_uri=args.tracking_uri)

    # ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ì‹œ ë”ë¯¸ ë°ì´í„° ìƒì„±
    if processed is None:
        print("[fe] Processed data not found, creating dummy data...")
        processed = create_dummy_processed_data()

    # ğŸ”¥ ìµœê·¼ Nì¼ë¡œ ìë¥´ê¸°
    try:
        processed = _slice_last_n_days(processed, args.train_window_days, "date")
        print(f"[fe] Sliced to last {args.train_window_days}d: {processed['date'].min()} ~ {processed['date'].max()}  -> {len(processed)} rows")
    except Exception as e:
        print(f"[fe] slice warning: {e}")

    # íƒ€ê²Ÿ ì»¬ëŸ¼ í™•ì¸
    if args.target not in processed.columns:
        print(f"[fe] Target column '{args.target}' not found. Available columns: {list(processed.columns)}")

        # ê¸°ë³¸ íƒ€ê²Ÿ ì»¬ëŸ¼ ìƒì„±
        if 'new_cases' not in processed.columns:
            import numpy as np
            processed['new_cases'] = np.random.randint(100, 1000, len(processed))
            print("[fe] Created dummy 'new_cases' column")

        args.target = 'new_cases'

    # íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì‹¤í–‰
    try:
        from src.feature_engineering.feature_engineer import CovidFeatureEngineer
        fe = CovidFeatureEngineer(tracking_uri=args.tracking_uri)

        features_df, target_series = fe.engineer_features(
            processed,
            target_column=args.target,
            run_name=args.run_name
        )

        print(f"[fe] Feature engineering completed successfully!")
        print(f"[fe] Features shape: {features_df.shape}")
        print(f"[fe] Target shape: {len(target_series)}")

        # â­ ë¡œì»¬ íŒŒì¼ë¡œ ì €ì¥ (ê¶Œí•œ ì—ëŸ¬ ì²˜ë¦¬)
        try:
            if args.output.startswith("s3://"):
                saved_path = _save_csv_to_s3(features_df, args.output)  # â† S3 ì—…ë¡œë“œ
                print(f"[fe] Features saved to S3: {saved_path}")
            else:
                output_dir = os.path.dirname(args.output)
                if output_dir and not os.path.exists(output_dir):
                    os.makedirs(output_dir, exist_ok=True)
                features_df.to_csv(args.output, index=False)  # â† ë¡œì»¬ ì €ì¥
                print(f"[fe] Features saved to: {args.output}")
        except PermissionError as pe:
            print(f"[fe] Permission error saving to {args.output}: {pe}")
            fallback_path = "/tmp/features.csv"
            features_df.to_csv(fallback_path, index=False)
            print(f"[fe] Features saved to fallback location: {fallback_path}")

    except Exception as e:
        print(f"[fe] Feature engineering failed: {e}")

        import traceback
        traceback.print_exc()  # â­ ì „ì²´ ì—ëŸ¬ ìŠ¤íƒ ì¶œë ¥

        # í´ë°±: ê°„ë‹¨í•œ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§
        print("[fe] Fallback to simple feature engineering...")
        import mlflow
        import pandas as pd
        import numpy as np

        if args.tracking_uri:
            mlflow.set_tracking_uri(args.tracking_uri)

        mlflow.set_experiment("covid_feature_engineering")

        # ê¸°ì¡´ í™œì„± ëŸ° ì •ë¦¬
        if mlflow.active_run():
            mlflow.end_run()

        with mlflow.start_run(run_name=args.run_name):
            # ê°„ë‹¨í•œ íŠ¹ì„± ìƒì„±
            features_df = processed.copy()

            # íƒ€ê²Ÿ ë¶„ë¦¬
            target_series = features_df[args.target].copy()

            # ê¸°ë³¸ ë¼ê·¸ íŠ¹ì„± ì¶”ê°€
            for lag in [1, 7, 14]:
                features_df[f'{args.target}_lag_{lag}'] = features_df[args.target].shift(lag)

            # ì´ë™í‰ê·  íŠ¹ì„± ì¶”ê°€
            for window in [7, 14]:
                features_df[f'{args.target}_rolling_mean_{window}'] = features_df[args.target].rolling(window).mean()

            # ê²°ì¸¡ì¹˜ ì œê±°
            features_df = features_df.dropna()
            target_series = target_series.loc[features_df.index]

            # íƒ€ê²Ÿ ì»¬ëŸ¼ ì œê±°
            if args.target in features_df.columns:
                features_df = features_df.drop(columns=[args.target])

            # MLflow ë¡œê¹…
            mlflow.log_param("feature_method", "simple_fallback")
            mlflow.log_metric("features_count", features_df.shape[1])
            mlflow.log_metric("samples_count", len(features_df))

            # ì•„í‹°íŒ©íŠ¸ ì €ì¥
            from datetime import datetime
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

            features_df.to_csv(f"/tmp/features_{timestamp}.csv", index=False)
            target_series.to_csv(f"/tmp/target_{timestamp}.csv", index=False, header=['target'])

            mlflow.log_artifact(f"/tmp/features_{timestamp}.csv", "features")
            mlflow.log_artifact(f"/tmp/target_{timestamp}.csv", "targets")

            # ì •ë¦¬
            os.remove(f"/tmp/features_{timestamp}.csv")
            os.remove(f"/tmp/target_{timestamp}.csv")

            print(f"[fe] Simple feature engineering completed: {features_df.shape}")

            # í´ë°± except ë¸”ë¡ì˜ ë§ˆì§€ë§‰
            try:
                features_df.to_csv(args.output, index=False)  # â­ íƒ€ê²Ÿ ì—†ì´
                print(f"[fe] Fallback features saved to: {args.output}")
            except Exception as save_error:
                fallback_path = "/tmp/features.csv"
                features_df.to_csv(fallback_path, index=False)  # â­ íƒ€ê²Ÿ ì—†ì´
                print(f"[fe] Features saved to final fallback: {fallback_path}")


if __name__ == "__main__":
    main()